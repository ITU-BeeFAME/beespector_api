# --- START OF FULL REPLACEMENT for beespector_api/main.py ---
from typing import List, Dict, Any, Tuple
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, ConfigDict
import pandas as pd
import joblib
import os
import json
import numpy as np # For NaN handling if any

# --- Configuration & Paths ---
DATA_DIR = "data"
MODEL_DIR = "models"
DATA_FILE = os.path.join(DATA_DIR, "adult.csv")
MODEL_FILE = os.path.join(MODEL_DIR, "base_adult_logistic_model.pkl")
COLUMNS_FILE = os.path.join(MODEL_DIR, "adult_model_columns.json")

# --- Global Variables to hold loaded data and model (loaded at startup) ---
dataset_df: pd.DataFrame = None
model_pipeline: Any = None
model_columns_info: Dict[str, List[str]] = None
# This will store the original data with an 'id' column and pre-calculated initial predictions
# It will be a list of dictionaries, where each dict matches frontend's InitialDataPoint (for GET)
initial_datapoints_cache: List[Dict[str, Any]] = []


# --- Pydantic Models (mostly the same, ensure features model matches actual features) ---

# This model MUST match the feature names EXACTLY as they are in your adult.csv
# AFTER your cleaning in train_model.py (e.g., 'hours_per_week', 'native_country')
# And as expected by your saved model_columns.json
class FeaturesFromDataset(BaseModel):
    age: int
    workclass: str
    fnlwgt: int
    education: str
    education_num: int
    marital_status: str
    occupation: str
    relationship: str
    race: str
    sex: str
    capital_gain: int
    capital_loss: int
    hours_per_week: int # Assuming this is hours_per_week after your cleaning
    native_country: str # Assuming this is native_country after your cleaning

    # If your actual CSV has "hours-per-week" and "native-country" (with hyphens)
    # AND your model_columns.json also lists them with hyphens, then use aliases here:
    # hours_per_week: int = Field(alias="hours-per-week")
    # native_country: str = Field(alias="native-country")

    model_config = ConfigDict(populate_by_name=True)


class InitialDataPoint(BaseModel): # For GET /api/datapoints response
    id: int
    x1: float # We need to decide which features map to x1, x2 for scatter plot
    x2: float # For now, let's use 'age' for x1 and 'hours_per_week' for x2
    true_label: int # This will be the 'target' column from the dataset
    features: FeaturesFromDataset # Use the model reflecting dataset features
    pred_label: int # Base model's initial prediction
    pred_prob: float  # Base model's initial prediction probability
    mitigated_pred_label: int
    mitigated_pred_prob: float
    model_config = ConfigDict(populate_by_name=True)

class EvaluatedPointPrediction(BaseModel):
    pred_label: int
    pred_prob: float

class EvaluatedPointData(BaseModel): # For PUT /api/datapoints/{id}/evaluate response
    id: int
    x1: float
    x2: float
    features: FeaturesFromDataset # Use the model reflecting dataset features
    true_label: int
    base_model_prediction: EvaluatedPointPrediction
    mitigated_model_prediction: EvaluatedPointPrediction
    model_config = ConfigDict(populate_by_name=True)

# --- Helper Functions ---

def get_predictions(input_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
    """Gets predictions and probabilities from the loaded model pipeline."""
    if model_pipeline is None:
        raise RuntimeError("Model not loaded.")
    try:
        # Model pipeline expects a DataFrame with columns in the order it was trained on.
        # The preprocessor within the pipeline handles categorical encoding etc.
        # Ensure input_df has the columns listed in model_columns_info['all_features_in_order']
        # and in that specific order.
        ordered_input_df = input_df[model_columns_info['all_features_in_order']]

        pred_probs_all_classes = model_pipeline.predict_proba(ordered_input_df)
        pred_probs_positive_class = pred_probs_all_classes[:, 1] # Probability of class '1'
        pred_labels = model_pipeline.predict(ordered_input_df)
        return pred_labels, pred_probs_positive_class
    except Exception as e:
        print(f"Error during prediction: {e}")
        # Fallback or raise more specific error
        # For now, return dummy predictions if error occurs
        dummy_labels = np.zeros(len(input_df), dtype=int)
        dummy_probs = np.zeros(len(input_df), dtype=float)
        return dummy_labels, dummy_probs


def simulate_mitigated_predictions(base_labels: np.ndarray, base_probs: np.ndarray, features_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
    """Simulates mitigated predictions based on base predictions and some feature (e.g., 'sex')."""
    mitigated_probs = base_probs.copy()
    
    # Example simulation: If 'sex' is 'Female', slightly reduce probability of positive class
    # This is a very basic simulation.
    if 'sex' in features_df.columns:
        female_condition = features_df['sex'].str.strip() == 'Female' # Ensure whitespace is handled if present
        mitigated_probs[female_condition] = np.clip(mitigated_probs[female_condition] * 0.9, 0.01, 0.99)
    
    # Example simulation: If 'race' is 'Black', slightly increase probability (opposite effect for demo)
    if 'race' in features_df.columns:
        black_condition = features_df['race'].str.strip() == 'Black'
        mitigated_probs[black_condition] = np.clip(mitigated_probs[black_condition] * 1.1, 0.01, 0.99)

    mitigated_labels = (mitigated_probs > 0.5).astype(int)
    return mitigated_labels, np.round(mitigated_probs, 3)

# --- FastAPI Lifespan Events (for loading data/model at startup) ---
app = FastAPI(title="Beespector API")

@app.on_event("startup")
async def startup_event():
    global dataset_df, model_pipeline, model_columns_info, initial_datapoints_cache
    print("Loading dataset and model at startup...")
    try:
        if not os.path.exists(DATA_FILE):
            raise FileNotFoundError(f"Dataset file not found: {DATA_FILE}")
        temp_df = pd.read_csv(DATA_FILE)
        temp_df.columns = temp_df.columns.str.strip().str.replace('-', '_', regex=False).str.replace('.', '_', regex=False)

        target_column_name = 'income_per_year'
        possible_targets = ['income', 'class', 'target', 'income_per_year']
        actual_target_col = None
        for ct_name in possible_targets:
            if ct_name in temp_df.columns:
                actual_target_col = ct_name
                print(f"Using target column: {actual_target_col}")
                break
        if not actual_target_col:
            raise ValueError(f"Target column for income not found. Checked for {possible_targets}")
        
        temp_df['target'] = temp_df[actual_target_col].apply(lambda x: 1 if str(x).strip() == '>50K' else 0)
        
        # Use all columns except the original string target and the new numeric 'target' for features initially
        # The actual features used for model prediction will be filtered by model_columns_info later
        dataset_df = temp_df.drop(columns=[actual_target_col]) 
        dataset_df = dataset_df.reset_index().rename(columns={'index': 'id'})

        if not os.path.exists(COLUMNS_FILE):
            raise FileNotFoundError(f"Model columns file not found: {COLUMNS_FILE}")
        with open(COLUMNS_FILE, 'r') as f:
            model_columns_info = json.load(f)

        # Validate required keys in model_columns_info
        for key in ['categorical_features', 'numerical_features', 'all_features_in_order']:
            if key not in model_columns_info:
                raise ValueError(f"Key '{key}' missing in {COLUMNS_FILE}")

        missing_cols_in_df = [col for col in model_columns_info['all_features_in_order'] if col not in dataset_df.columns]
        if missing_cols_in_df:
            print(f"Loaded dataset columns: {dataset_df.columns.tolist()}")
            raise ValueError(f"Dataset is missing columns required by the model (from model_columns_info): {missing_cols_in_df}")

        if not os.path.exists(MODEL_FILE):
            raise FileNotFoundError(f"Model file not found: {MODEL_FILE}")
        model_pipeline = joblib.load(MODEL_FILE)
        print("Model loaded successfully.")

        # For performance on startup, let's process a sample for the cache first, then maybe all if needed later.
        # OR, if we only ever show a sample, just process the sample.
        # For now, let's sample dataset_df BEFORE intensive predictions
        SAMPLE_SIZE_FOR_CACHE = 500 # Reduced sample size for faster startup and UI
        if len(dataset_df) > SAMPLE_SIZE_FOR_CACHE:
            print(f"Dataset has {len(dataset_df)} rows. Sampling {SAMPLE_SIZE_FOR_CACHE} for initial cache and API.")
            df_for_cache = dataset_df.sample(n=SAMPLE_SIZE_FOR_CACHE, random_state=42).copy()
        else:
            df_for_cache = dataset_df.copy()
        
        df_for_cache = df_for_cache.reset_index(drop=True) # Important after sampling for iloc later

        # Prepare features from the sampled df_for_cache
        features_for_prediction_df = df_for_cache[model_columns_info['all_features_in_order']].copy()

        for col in features_for_prediction_df.columns:
            if features_for_prediction_df[col].isnull().any():
                if pd.api.types.is_numeric_dtype(features_for_prediction_df[col]):
                    fill_value = features_for_prediction_df[col].median()
                else:
                    fill_value = features_for_prediction_df[col].mode()[0] if not features_for_prediction_df[col].mode().empty else "Unknown"
                features_for_prediction_df[col] = features_for_prediction_df[col].fillna(fill_value)
        
        base_pred_labels_cache, base_pred_probs_cache = get_predictions(features_for_prediction_df)
        # Pass features_for_prediction_df which corresponds to the base_pred_labels/probs
        mitigated_pred_labels_cache, mitigated_pred_probs_cache = simulate_mitigated_predictions(
            base_pred_labels_cache, base_pred_probs_cache, features_for_prediction_df
        )

        x1_col, x2_col = 'age', 'hours_per_week' # Default scatter plot axes
        # ... (logic to pick x1_col, x2_col if default not available, remains same) ...
        if x1_col not in model_columns_info['numerical_features'] or x2_col not in model_columns_info['numerical_features']:
            print(f"Warning: Default x1 ('{x1_col}') or x2 ('{x2_col}') for scatter plot not in numerical features. Using first two numerical features instead.")
            if len(model_columns_info['numerical_features']) >= 2:
                x1_col = model_columns_info['numerical_features'][0]
                x2_col = model_columns_info['numerical_features'][1]
            elif len(model_columns_info['numerical_features']) == 1:
                x1_col = model_columns_info['numerical_features'][0]; x2_col = x1_col 
            else: x1_col, x2_col = 'age', 'hours_per_week'; print("Error: Not enough numerical features for x1, x2.")


        for i in range(len(df_for_cache)): # Iterate over the sampled df_for_cache
            row = df_for_cache.iloc[i] # Use df_for_cache here
            
            current_features_dict = {}
            for feature_name in model_columns_info['all_features_in_order']:
                current_features_dict[feature_name] = row[feature_name]
            
            for k, v in current_features_dict.items():
                if isinstance(v, np.integer): current_features_dict[k] = int(v)
                elif isinstance(v, np.floating): current_features_dict[k] = float(v)
                elif pd.isna(v):
                    is_cat = isinstance(model_columns_info['categorical_features'], list) and k in model_columns_info['categorical_features']
                    is_num = isinstance(model_columns_info['numerical_features'], list) and k in model_columns_info['numerical_features']
                    current_features_dict[k] = "Unknown" if is_cat else (0 if is_num else None)
            
            initial_datapoints_cache.append({
                "id": int(row['id']),
                "x1": float(row[x1_col]),
                "x2": float(row[x2_col]),
                "true_label": int(row['target']),
                "features": current_features_dict,
                "pred_label": int(base_pred_labels_cache[i]),
                "pred_prob": float(base_pred_probs_cache[i]),
                "mitigated_pred_label": int(mitigated_pred_labels_cache[i]), # Correctly use from per-point simulation
                "mitigated_pred_prob": float(mitigated_pred_probs_cache[i])  # Correctly use from per-point simulation
            })
        
        print(f"Dataset loaded/processed. {len(initial_datapoints_cache)} points in API cache for GET /api/datapoints.")
        if initial_datapoints_cache:
             print(f"Example initial datapoint features for ID {initial_datapoints_cache[0]['id']}: {initial_datapoints_cache[0]['features']}")

    except FileNotFoundError as e: print(f"ERROR during startup: {e}.")
    except ValueError as e: print(f"ERROR during startup (ValueError): {e}.")
    except Exception as e:
        print(f"UNEXPECTED ERROR during startup: {e}.")
        import traceback
        traceback.print_exc()

# --- CORS Middleware (remains the same) ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["Accept", "Authorization", "Content-Type", "X-Requested-With"],
)

# --- API Endpoints ---
@app.get("/api/datapoints", response_model=Dict[str, List[InitialDataPoint]])
async def get_all_datapoints():
    if not initial_datapoints_cache:
        # This might happen if startup failed catastrophically before cache was populated
        # Or if you want to load data on-demand (not recommended for this app)
        print("Warning: initial_datapoints_cache is empty. Startup might have failed.")
        return {"data": []}
    
    # Convert list of dicts to list of Pydantic model instances for validation and response
    # Pydantic will use InitialDataPoint's FeaturesFromDataset for the 'features' field
    # and correctly serialize using aliases if FeaturesFromDataset is defined with them.
    try:
        data_to_send = [InitialDataPoint(**dp_dict) for dp_dict in initial_datapoints_cache]
        return {"data": data_to_send}
    except Exception as e:
        print(f"Error creating InitialDataPoint instances: {e}")
        raise HTTPException(status_code=500, detail="Error processing data for response")


class UpdatePointPayloadFeatures(BaseModel): # For parsing features from PUT request body
    # This MUST match the structure of FeaturesFromDataset (or have appropriate aliases
    # if frontend sends different keys than Pythonic names)
    age: int
    workclass: str
    fnlwgt: int # Not used in current mock predictions but part of dataset
    education: str
    education_num: int # Not used in current mock predictions
    marital_status: str
    occupation: str
    relationship: str
    race: str
    sex: str
    capital_gain: int
    capital_loss: int
    hours_per_week: int
    native_country: str
    model_config = ConfigDict(populate_by_name=True)
    # If frontend sends "hours-per-week", "native-country", use Field(validation_alias=...)
    # For now, assume frontend sends pythonic names or FeaturesFromDataset handles it via aliases


class UpdatePointPayload(BaseModel): # For the whole PUT request body
    x1: float # Corresponds to 'age' in this setup
    x2: float # Corresponds to 'hours_per_week' in this setup
    features: UpdatePointPayloadFeatures # This is the full feature set
    model_config = ConfigDict(populate_by_name=True)


@app.put("/api/datapoints/{point_id}/evaluate", response_model=EvaluatedPointData)
async def evaluate_modified_point(point_id: int, payload: UpdatePointPayload):
    if dataset_df is None or model_pipeline is None or model_columns_info is None:
        raise HTTPException(status_code=503, detail="Model or dataset not loaded. API not ready.")

    # Find the original true_label for the point ID
    original_row = dataset_df[dataset_df['id'] == point_id]
    if original_row.empty:
        raise HTTPException(status_code=404, detail="Point ID not found in dataset.")
    true_label = int(original_row.iloc[0]['target'])

    # Create a single-row DataFrame from the payload features for prediction
    # The keys in payload.features should match the actual feature names used by the model
    # (after any cleaning like underscore conversion)
    try:
        # payload.features is an instance of UpdatePointPayloadFeatures
        # Convert its attributes to a dictionary
        features_dict_for_df = payload.features.model_dump() # Use Pydantic's method
        
        # Ensure keys in features_dict_for_df match your model's expected column names
        # e.g., if model expects "hours_per_week", ensure dict has that.
        # The Pydantic models should align here.

        input_features_df = pd.DataFrame([features_dict_for_df])
    except Exception as e:
        print(f"Error creating DataFrame from payload features: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid features structure in payload: {e}")


    # Ensure all necessary columns are present, fill if any are missing from payload features
    # (though Pydantic model should enforce this)
    for col in model_columns_info['all_features_in_order']:
        if col not in input_features_df.columns:
            # This case should ideally be caught by Pydantic validation of UpdatePointPayloadFeatures
            print(f"Warning: Feature '{col}' missing in PUT payload, adding with default/NaN.")
            input_features_df[col] = np.nan # Or a suitable default

    # Handle NaNs in the input_features_df (e.g., from missing optional fields)
    for col in input_features_df.columns:
        if input_features_df[col].isnull().any():
            if pd.api.types.is_numeric_dtype(input_features_df[col]):
                fill_value = dataset_df[col].median() # Use median from original dataset
            else:
                fill_value = dataset_df[col].mode()[0] if not dataset_df[col].mode().empty else "Unknown"
            input_features_df[col] = input_features_df[col].fillna(fill_value)


    base_pred_labels, base_pred_probs = get_predictions(input_features_df)
    mitigated_pred_labels, mitigated_pred_probs = simulate_mitigated_predictions(base_pred_labels, base_pred_probs, input_features_df)
    
    # Prepare features for response (should be instance of FeaturesFromDataset)
    # payload.features is UpdatePointPayloadFeatures. Convert to FeaturesFromDataset if different,
    # or ensure they are compatible. For now, assuming they are structurally similar.
    # We can directly use features_dict_for_df if its structure matches FeaturesFromDataset for output.
    response_features = FeaturesFromDataset(**features_dict_for_df)


    return EvaluatedPointData(
        id=point_id,
        x1=payload.x1, # This is what the frontend uses for scatter x-axis
        x2=payload.x2, # This is what the frontend uses for scatter y-axis
        features=response_features, # Send back the features used for prediction
        true_label=true_label,
        base_model_prediction=EvaluatedPointPrediction(pred_label=int(base_pred_labels[0]), pred_prob=float(base_pred_probs[0])),
        mitigated_model_prediction=EvaluatedPointPrediction(pred_label=int(mitigated_pred_labels[0]), pred_prob=float(mitigated_pred_probs[0]))
    )

# --- Placeholder Endpoints (remain the same for now) ---
@app.get("/api/partial_dependence")
async def get_partial_dependence(): return {"partial_dependence_data": []}
@app.get("/api/performance_fairness")
async def get_performance_fairness(): return { "roc_curve": [], "pr_curve": [], "confusion_matrix": {"tn": 0, "fp": 0, "fn": 0, "tp": 0}, "fairness_metrics": {"StatisticalParityDiff": 0, "DisparateImpact": 0, "EqualOpportunityDiff": 0}, "performance_metrics": {"Accuracy": 0, "F1Score": 0, "AUC": 0}}
@app.get("/api/features")
async def get_features(): return {"features": []}

# --- END OF FULL REPLACEMENT for beespector_api/main.py ---